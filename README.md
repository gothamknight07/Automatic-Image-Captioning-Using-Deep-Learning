# Automatic Image Captioning with Deep Learning


## Introduction
Google released the ‘Google’s Conceptual Captions’ dataset for image captioning as a new image-recognition challenge and an exercise in AI-driven education. Since pictures may convey a great deal of information, it is important that we create ways of generating conceptual captions from images automatically and accurately.

This project focus on combining current state-of-the-art techniques in both computer vision and natural language processing in a single jointly-trained system which takes images as inputs and generates human-readable descriptions of them.

## Objective
Develop a useful Deep Learning application which combines both computer vision and natural language processing to create accurate, comprehensible captions from images alone.

Understand the various ways in which we can train Neural Networks more quickly through the use of optimizers?

Understand what’s going on in the model, detailing the inter-relations between components.



## Model Structure
The task of image captioning can be divided into two models:

    • Image Based Model: extract features from an image
    • Language Based Model: create a sentence about the picture using previous captions together with features given by our image based model.

Below are the details of the above two models:

### Image Based Model – Convolutional Neural Network
I utilize transfer learning to generate image features by feeding the images into a pre-trained VGG16 model. This popular model was developed by Oxford's Visual Geometry Group and in recent times has been used in a variety of image-processing tasks.

### Language Based Model – Recurrent Neural Network
Caption sequences were generated by inputting text into a word embedding layer and feeding the resultant output into a subsequent long short-term memory (LSTM) layer.

LSTM cells are a particular kind of recurrent neural net (RNN) with a more complicated set of internal weights which make them both better able to learn long-term dependencies and less susceptible to the vanishing/exploding gradient problem.


## Data Engineering
I created the following three functions for cleaning my existing set of captions:

    • remove punctuation
    • remove single character
    • remove numeric characters

After cleaning, the vocabulary size was reduced by about 200 tokens, where each token is just a unique word. A word cloud was generated in order to easily determine which words and phrases were the most common.

## Image Based Model - CNN

### Create features for each image using VGG16's pre-trained networks

Because the VGG16’s pre-trained network requires an input of size (224,224,3), I had to resize all the input images to the same dimensions. Next these re-sized images were sent through the network and encoded into a 4096 size array. These arrays were re-sized once again so as to have the same dimension as the inputs. If this procedure is hard to follow, consult the flowchart below.


### PCA Visualization of the VGG16 features
For each image, 4096 features are created, reflecting the dimensions of the array passed through the network. Because humans lack the circuitry required to visualize a 4096-dimensional space, I created 2-dimensional representation of the space using PCA and visualized the distribution of the sample images.

### Do photo features make sense?
I used k-means clustering on the VGG16 features to separate the images into 4 different clusters.

I then plotted images from the purple and blue cluster. As you can see, images from the blue cluster tend to resemble each other and images from the purple cluster to resemble each other.

I therefore submit that this clustering schema makes sense.

## Language Based Model – RNN

My goal is to build a Language Based Model using an RNN. Here’s what that means: for a given sequence of words of length *n*, the model has a set of probabilities corresponding to predictions about what the next word *n + 1* will be. It chooses the highest-probability word from this distribution and continues this process until it generates the special end marker “endseq”, which appears at the end of each sentence in the corpus.

RNNs are sometimes depicted as being "unfolded" to their full extent. By 'unfolding' we simply mean that we write out the network as it would appear as it processes an input. For example, image that we're dealing with a sentence consisting of 5 words. In this case I would vectorize these words, producing five vectors, each of which would be paired with the input image and fed into the corresponding cell of the RNN.

The first cell in the RNN would get just the image (we're not predicting words yet), and would output 'startseq', the special sequence which begins every sentence in the corpus. The second cell would receive the image together with 'startseq', and would predict the first word in the sentence. The third cell would receive the image, 'startseq', plus the first predicted word, and predict the second word. The fourth cell would receive the image, 'startseq', and the first two words, and predict the third word...


## How to train Neural Networks more quickly through the use of optimizer?
Optimization refers to searching the space of possible parameters that minimize or maximize some function that we care about.

Typically training a machine learning model involves "indirect" optimization, wherein we pick a metric such as like recall or accuracy to quantify how well our model solves the problem we're interest in. However, we are actually optimizing a cost function which measures the errors in our predictions and hoping that this also moves our chosen metric in the direction we want it to go [3].


### Gradient Descent
One way we can achieve this is to take our cost function, calculate its gradient, and try to 'descend' the gradient, a process captured by the following equation:

Here, the learning rate 'α' describes how much we will change our weights with each iteration of training. Smaller learning rates means we have to do more calculations, but a big one makes it possible that we will miss the minimum of the cost function altogether.

### Gradient Descent with Momentum
Let's take rolling a ball downhill as an example. Adding momentum term (*V_dw* and *V_db*) to the descent is make the ball roll faster and faster. Beta is used to prevent the ball speeding up without limit. It is just a way to make the gradient descent happen faster by skipping over the steeper parts of the learning curve.

### Root Mean Squared Propagation (RMSProp)
Another way of approaching gradient descent is to use Root Mean Squared Propagation, or 'RMSProp', which has come into frequent use in recent years. It uses adaptive learning rate to help pointing the correct direction towards the minimum.

### Adaptive Moment Estimation (Adam)
Finally, Adaptive Moment Estimation, or "Adam", is a popular choice for gradient descent because it is a powerful blend of RMSProp and descent with momentum:

## Model Result
Figure 11 shows the loss trends for both the training data and the testing data through using different optimizers. It is apparently Adaptive Moment Estimation (Adam) optimizer outperformed other optimizers in this case.

I discovered that my model's loss on training data continued to decrease even its loss on evaluation data increased when using Adam. For navigation, I wrote a function to store the model weights for each epoch after they had been used to make predictions. This way I could visually determine if the model was overfitting and the results were being skewed. Below table lists the predicted captions at selected epoch for random chosen images.

## Model Evaluation
### Bilingual Evaluation Understudy (BLEU)
For testing purposes I turned to the Bilingual evaluation understudy (BLUE) set of benchmarks. BLEU contained a metric for determining the similarity between a model-generated sentence and various other reference sentences. Being close to 1 means that the two are very similar, being close to 0 means they are very dissimilar[4].

### Basic Idea of BLEU
For example let's consider a scenario in which I want to compare the similarly between the hypothesis sentence and the reference sentences.

Finally, 1-gram BLEU, 2-gram BLEU, 3-gram BLEU and 4-gram BLEU are calculated and its average is reported as BLEU. BLEU score calculation is implemented in nltk.util and the source code is available.


## Conclusion
This project focused on building a system to generate captions for images. I used a pre-trained image-model (VGG16) to generate a "thought-vector" of the image, and then I trained a Recurrent Neural Network to map this "thought-vector" to a sequence of integers representing words.

I also explored various ways in which we can train Neural Networks more quickly through the use of optimizers. It turned out that the Adaptive Moment Estimation (Adam) optimizer outperformed other optimizers in this case.

This model works reasonably well, although it is easy to find examples both in the training- and validation-sets where the captions are incorrect.

It's crucial to bear in mind that this model doesn't have a semantic understanding of the content of the images. If it sees an image of a dog and correctly produces a caption stating that, it doesn't mean that the model has anything corresponding to the 'dog' concept.
